{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# GDACS Earthquake Event Scraper\n",
    "## Assignment 1 — Disaster Study Module\n",
    "\n",
    "This notebook scrapes earthquake event data from the [Global Disaster Alert and Coordination System (GDACS)](https://www.gdacs.org) using **Selenium** with a headless Safari/Chrome browser (no explicit ChromeDriver required — uses `selenium-manager` auto-provisioning introduced in Selenium 4.6+).\n",
    "\n",
    "### Events Scraped\n",
    "| Label | Country | Type | URL |\n",
    "|---|---|---|---|\n",
    "| Philippines – Historical | Philippines | EQ | eventid=1230629 |\n",
    "| Philippines – Recent | Philippines | EQ | eventid=1502713 |\n",
    "| Afghanistan – Historical | Afghanistan | EQ | eventid=1327560 |\n",
    "| Afghanistan – Recent | Afghanistan | EQ | eventid=1508467 |\n",
    "\n",
    "### Data Extracted (per event)\n",
    "- **Summary Tab**: Event title, magnitude, depth, event date (UTC), GDACS score, alert level, country, exposed population (summary)\n",
    "- **Impact Tab (Shakemap)**: Magnitude, depth, event date, exposed population (detailed)\n",
    "- **Impact Tab (INFORM)**: INFORM coping capacity score, vulnerability score\n",
    "- **Media Tab**: Total articles, articles about casualties, articles in last hour, peak news day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Cell 1 — Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n",
      "Selenium version: 4.40.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "print('Imports successful.')\n",
    "import selenium; print(f'Selenium version: {selenium.__version__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## Cell 2 — Event Configuration\n",
    "\n",
    "All four GDACS event URLs and their metadata labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events configured:\n",
      "  • Philippines – Historical → https://www.gdacs.org/report.aspx?eventid=1230629&episodeid=1327163&eventtype=EQ\n",
      "  • Philippines – Recent → https://www.gdacs.org/report.aspx?eventid=1502713&episodeid=1663312&eventtype=EQ\n",
      "  • Afghanistan – Historical → https://www.gdacs.org/report.aspx?eventid=1327560&episodeid=1449790&eventtype=EQ\n",
      "  • Afghanistan – Recent → https://www.gdacs.org/report.aspx?eventid=1508467&episodeid=1669785&eventtype=EQ\n"
     ]
    }
   ],
   "source": [
    "EVENTS = [\n",
    "    {\n",
    "        \"label\":   \"Philippines – Historical\",\n",
    "        \"country\": \"Philippines\",\n",
    "        \"period\":  \"Historical\",\n",
    "        \"url\":     \"https://www.gdacs.org/report.aspx?eventid=1230629&episodeid=1327163&eventtype=EQ\",\n",
    "    },\n",
    "    {\n",
    "        \"label\":   \"Philippines – Recent\",\n",
    "        \"country\": \"Philippines\",\n",
    "        \"period\":  \"Recent\",\n",
    "        \"url\":     \"https://www.gdacs.org/report.aspx?eventid=1502713&episodeid=1663312&eventtype=EQ\",\n",
    "    },\n",
    "    {\n",
    "        \"label\":   \"Afghanistan – Historical\",\n",
    "        \"country\": \"Afghanistan\",\n",
    "        \"period\":  \"Historical\",\n",
    "        \"url\":     \"https://www.gdacs.org/report.aspx?eventid=1327560&episodeid=1449790&eventtype=EQ\",\n",
    "    },\n",
    "    {\n",
    "        \"label\":   \"Afghanistan – Recent\",\n",
    "        \"country\": \"Afghanistan\",\n",
    "        \"period\":  \"Recent\",\n",
    "        \"url\":     \"https://www.gdacs.org/report.aspx?eventid=1508467&episodeid=1669785&eventtype=EQ\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Output directory for the cleaned CSV\n",
    "OUTPUT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"gdacs_earthquake_data.csv\")\n",
    "\n",
    "print(\"Events configured:\")\n",
    "for e in EVENTS:\n",
    "    print(f\"  • {e['label']} → {e['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## Cell 3 — Selenium Driver Setup\n",
    "\n",
    "Uses **Selenium 4.6+ built-in `selenium-manager`** which automatically downloads the correct ChromeDriver for the installed Chrome version — no manual `chromedriver` installation required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing driver creation...\n",
      "Driver created successfully. User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\n",
      "Driver closed. Ready to scrape.\n"
     ]
    }
   ],
   "source": [
    "def create_driver(headless: bool = True) -> webdriver.Chrome:\n",
    "    \"\"\"\n",
    "    Create and return a Chrome WebDriver instance.\n",
    "    \n",
    "    Selenium 4.6+ includes selenium-manager which automatically\n",
    "    downloads the correct ChromeDriver — no manual driver needed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    headless : bool\n",
    "        If True, Chrome runs without a visible window (default).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    webdriver.Chrome\n",
    "    \"\"\"\n",
    "    options = ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")          # 'new' headless mode (Chrome 112+)\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\n",
    "        \"--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    # selenium-manager handles driver download automatically\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.implicitly_wait(5)\n",
    "    return driver\n",
    "\n",
    "\n",
    "# Quick smoke-test\n",
    "print(\"Testing driver creation...\")\n",
    "_test = create_driver(headless=True)\n",
    "print(f\"Driver created successfully. User-Agent: {_test.execute_script('return navigator.userAgent')}\")\n",
    "_test.quit()\n",
    "print(\"Driver closed. Ready to scrape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## Cell 4 — Helper: Safe Text Extraction\n",
    "\n",
    "Convenience functions to extract text from DOM elements without raising exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def safe_text(driver, xpath: str, default: str = 'N/A') -> str:\n",
    "    \"\"\"Return stripped text of the first XPATH match, or default.\"\"\"\n",
    "    try:\n",
    "        return driver.find_element(By.XPATH, xpath).text.strip()\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def safe_texts(driver, xpath: str) -> list:\n",
    "    \"\"\"Return list of stripped texts for all XPATH matches.\"\"\"\n",
    "    try:\n",
    "        return [el.text.strip() for el in driver.find_elements(By.XPATH, xpath)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def label_value(driver, label: str, default: str = 'N/A') -> str:\n",
    "    \"\"\"\n",
    "    Extract the value cell from a two-column label/value table.\n",
    "    Tries exact match first, then partial match.\n",
    "    \"\"\"\n",
    "    for xp in [\n",
    "        f\"//td[normalize-space(text())='{label}']/following-sibling::td[1]\",\n",
    "        f\"//td[contains(text(),'{label}')]/following-sibling::td[1]\",\n",
    "    ]:\n",
    "        val = safe_text(driver, xp)\n",
    "        if val and val != 'N/A':\n",
    "            return val\n",
    "    return default\n",
    "\n",
    "\n",
    "def build_urls(base_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    GDACS report pages are SEPARATE PAGES (not in-page tabs).\n",
    "    Given the base report URL, derive the Impact and Media page URLs.\n",
    "\n",
    "    URL patterns (confirmed from live DOM inspection):\n",
    "      Summary : https://www.gdacs.org/report.aspx?eventid=X&episodeid=Y&eventtype=EQ\n",
    "      Impact  : https://www.gdacs.org/Earthquakes/report_shakemap.aspx?eventid=X&episodeid=Y&eventtype=EQ\n",
    "      Media   : https://www.gdacs.org/media.aspx?eventid=X&episodeid=Y&eventtype=EQ\n",
    "    \"\"\"\n",
    "    # Parse eventid, episodeid, eventtype from query string\n",
    "    eid  = re.search(r'eventid=(\\d+)',   base_url).group(1)\n",
    "    epid = re.search(r'episodeid=(\\d+)', base_url).group(1)\n",
    "    etype = re.search(r'eventtype=(\\w+)', base_url).group(1)\n",
    "    qs = f'eventid={eid}&episodeid={epid}&eventtype={etype}'\n",
    "    return {\n",
    "        'summary': base_url,\n",
    "        'impact':  f'https://www.gdacs.org/Earthquakes/report_shakemap.aspx?{qs}',\n",
    "        'media':   f'https://www.gdacs.org/media.aspx?{qs}',\n",
    "    }\n",
    "\n",
    "\n",
    "print('Helper functions defined.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "## Cell 5 — Summary Tab Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape_summary() defined.\n"
     ]
    }
   ],
   "source": [
    "def scrape_summary(driver, url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Navigate to the Summary page and scrape all headline fields.\n",
    "\n",
    "    URL: report.aspx?eventid=X&episodeid=Y&eventtype=EQ\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # ─ Event Title (─ confirmed: div.alert_title) ─────────────────────────────\n",
    "    raw_title = safe_text(driver, \"//div[@class='alert_title']\")\n",
    "    if raw_title == 'N/A':\n",
    "        raw_title = safe_text(driver, \"//div[contains(@class,'alert_title')]\")\n",
    "    data['event_title'] = re.sub(r'\\s+', ' ', raw_title).strip()\n",
    "\n",
    "    # ─ Summary table (td label / td value pairs) ───────────────────────\n",
    "    data['gdacs_id']  = label_value(driver, 'GDACS ID')\n",
    "    data['magnitude'] = label_value(driver, 'Earthquake Magnitude:')\n",
    "    data['depth_km']  = label_value(driver, 'Depth:')\n",
    "    data['lat_lon']   = label_value(driver, 'Lat/Lon:')\n",
    "    raw_date = label_value(driver, 'Event Date:')\n",
    "    data['event_date_utc'] = raw_date.split('\\n')[0].strip() if raw_date != 'N/A' else 'N/A'\n",
    "    data['exposed_population_summary'] = label_value(driver, 'Exposed Population:')\n",
    "\n",
    "    # ─ GDACS Score (confirmed: last td in tbody#tableScoreMain data row) ───\n",
    "    gdacs_score = 'N/A'\n",
    "    try:\n",
    "        tbody = driver.find_element(By.XPATH, \"//tbody[@id='tableScoreMain']\")\n",
    "        rows  = tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "        if len(rows) >= 2:\n",
    "            tds = rows[1].find_elements(By.TAG_NAME, 'td')\n",
    "            if tds:\n",
    "                gdacs_score = tds[-1].text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not gdacs_score or gdacs_score == 'N/A':\n",
    "        gdacs_score = safe_text(driver, \"//td[contains(@class,'cell_matrix_gdacs')]\")\n",
    "    data['gdacs_score'] = gdacs_score\n",
    "\n",
    "    # ─ Alert Level (confirmed: page <title> = 'Overall Orange/Green/Red Earthquake...') ─\n",
    "    am = re.search(r'Overall\\s+(Green|Orange|Red)', driver.title, re.I)\n",
    "    if am:\n",
    "        data['alert_level'] = am.group(1).capitalize()\n",
    "    else:\n",
    "        try:\n",
    "            sv = float(gdacs_score)\n",
    "            data['alert_level'] = 'Green' if sv < 1.0 else ('Orange' if sv <= 2.0 else 'Red')\n",
    "        except (ValueError, TypeError):\n",
    "            data['alert_level'] = 'N/A'\n",
    "\n",
    "    # ─ Country (from event_title: 'M 6.7 in Philippines on 18 Aug...') ────────\n",
    "    cm = re.search(r'\\bin\\s+([A-Za-z ,()-]+?)\\s+on\\s', data.get('event_title', ''), re.I)\n",
    "    data['country'] = cm.group(1).strip() if cm else 'N/A'\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "print('scrape_summary() defined.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": [
    "## Cell 6 — Impact Tab Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape_impact() defined.\n"
     ]
    }
   ],
   "source": [
    "def scrape_impact(driver, url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Navigate to the Impact (Shakemap) page and scrape impact metrics.\n",
    "\n",
    "    URL: Earthquakes/report_shakemap.aspx?eventid=X&episodeid=Y&eventtype=EQ\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # ─ Core parameters ────────────────────────────────────────────────\n",
    "    data['impact_magnitude'] = label_value(driver, 'Earthquake Magnitude:')\n",
    "    data['impact_depth_km']  = label_value(driver, 'Depth:')\n",
    "    raw_date = label_value(driver, 'Event Date:')\n",
    "    data['impact_event_date_utc'] = raw_date.split('\\n')[0].strip() if raw_date != 'N/A' else 'N/A'\n",
    "\n",
    "    # ─ Exposed Population ──────────────────────────────────────────\n",
    "    # Two formats:\n",
    "    #   A: 'About 11000 people in MMI\\n2570000 people within 100km'\n",
    "    #   B: '860 thousand (in MMI>=VII)'\n",
    "    raw_pop = label_value(driver, 'Exposed Population:')\n",
    "    pop_mmi = pop_100km = 'N/A'\n",
    "    if raw_pop and raw_pop != 'N/A':\n",
    "        m1 = re.search(r'About\\s+([\\d,]+)\\s+people\\s+in\\s+MMI', raw_pop, re.I)\n",
    "        m2 = re.search(r'([\\d,.]+\\s*(?:thousand|million)?)\\s*\\(in\\s*MMI', raw_pop, re.I)\n",
    "        mk = re.search(r'([\\d,]+)\\s+people\\s+within\\s+100\\s*km', raw_pop, re.I)\n",
    "        pop_mmi   = m1.group(1).replace(',','').strip() if m1 else (m2.group(1).strip() if m2 else 'N/A')\n",
    "        pop_100km = mk.group(1).replace(',','').strip() if mk else 'N/A'\n",
    "    data['exposed_population_mmi']   = pop_mmi\n",
    "    data['exposed_population_100km'] = pop_100km\n",
    "\n",
    "    # ─ INFORM Coping Capacity ───────────────────────────────────────\n",
    "    inform_score = 'N/A'\n",
    "    for lbl in ('INFORM Coping capacity of the alert score:', 'INFORM Coping capacity', 'Coping capacity', 'INFORM'):\n",
    "        val = label_value(driver, lbl)\n",
    "        if val and val != 'N/A':\n",
    "            inform_score = val\n",
    "            break\n",
    "    data['inform_coping_capacity'] = inform_score\n",
    "\n",
    "    # ─ Vulnerability Score ─────────────────────────────────────────\n",
    "    vuln_score = 'N/A'\n",
    "    for lbl in ('Country Vulnerability', 'Vulnerability', 'Socio-economic vulnerability', 'Socio-economic'):\n",
    "        val = label_value(driver, lbl)\n",
    "        if val and val != 'N/A':\n",
    "            vuln_score = val\n",
    "            break\n",
    "    data['vulnerability_score'] = vuln_score\n",
    "\n",
    "    # ─ Tsunami Score ──────────────────────────────────────────────\n",
    "    # Score table (tbody#tableScoreMain): header tds use 'cell_type_matrix',\n",
    "    # data tds use cell_orange/green/red_matrix. Find Tsunami column by header text.\n",
    "    tsunami_score = 'N/A'\n",
    "    try:\n",
    "        tbody = driver.find_element(By.XPATH, \"//tbody[@id='tableScoreMain']\")\n",
    "        rows  = tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "        if len(rows) >= 2:\n",
    "            h_tds = rows[0].find_elements(By.TAG_NAME, 'td')\n",
    "            d_tds = rows[1].find_elements(By.TAG_NAME, 'td')\n",
    "            tsunami_col = next((i for i, td in enumerate(h_tds) if 'Tsunami' in td.text), None)\n",
    "            if tsunami_col is not None and len(d_tds) > tsunami_col:\n",
    "                tsunami_score = d_tds[tsunami_col].text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    data['tsunami_score'] = tsunami_score\n",
    "\n",
    "    # ─ Secondary Risks ───────────────────────────────────────────\n",
    "    secondary_els = driver.find_elements(\n",
    "        By.XPATH,\n",
    "        \"//td[ancestor::tbody and (\"\n",
    "        \"contains(normalize-space(.),'Secondary') or \"\n",
    "        \"contains(normalize-space(.),'Landslide') or \"\n",
    "        \"contains(normalize-space(.),'secondary hazard')\"\n",
    "        \")]/following-sibling::td[1]\"\n",
    "    )\n",
    "    risks = [el.text.strip() for el in secondary_els if el.text.strip()]\n",
    "    data['secondary_risks'] = '; '.join(risks) if risks else 'N/A'\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "print('scrape_impact() defined.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "## Cell 7 — Media Tab Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape_media() defined.\n"
     ]
    }
   ],
   "source": [
    "def scrape_media(driver, url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Navigate to the Media page (media.aspx) and scrape article counts and news data.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # ─ Media Coverage counts ─────────────────────────────────────────\n",
    "    data['total_articles']     = label_value(driver, 'Articles:')\n",
    "    data['casualty_articles']  = label_value(driver, 'Articles about casualties:')\n",
    "    data['articles_last_hour'] = label_value(driver, 'Articles in last hour:')\n",
    "\n",
    "    # ─ Peak News Day ──────────────────────────────────────────────\n",
    "    # div#newsForDay contains a nested table structure.\n",
    "    # Most reliable: BeautifulSoup parse with bar div title attributes.\n",
    "    # Bar div title format: '2020-08-21T00:00:00: 3'  (date: count)\n",
    "    date_cells  = []\n",
    "    count_cells = []\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    nfd  = soup.find('div', id='newsForDay')\n",
    "    if nfd:\n",
    "        for bar_div in nfd.find_all('div', title=True):\n",
    "            t   = bar_div.get('title', '')\n",
    "            cnt_m = re.search(r'(\\d{4}-\\d{2}-\\d{2})T[\\d:]+:\\s*(\\d+)$', t)\n",
    "            if cnt_m:\n",
    "                # Convert YYYY-MM-DD to DD/MM for consistency with GDACS display\n",
    "                raw_date = cnt_m.group(1)         # e.g. '2020-08-21'\n",
    "                parts    = raw_date.split('-')    # ['2020','08','21']\n",
    "                disp_date = f'{parts[2]}/{parts[1]}'  # '21/08'\n",
    "                date_cells.append(disp_date)\n",
    "                count_cells.append(cnt_m.group(2))\n",
    "\n",
    "    if date_cells and count_cells:\n",
    "        try:\n",
    "            ints = [int(c) for c in count_cells]\n",
    "            peak = ints.index(max(ints))\n",
    "            data['peak_news_day']   = date_cells[peak]\n",
    "            data['peak_news_count'] = str(max(ints))\n",
    "            data['news_per_day']    = '; '.join(f'{d}:{c}' for d, c in zip(date_cells, count_cells))\n",
    "        except (ValueError, IndexError):\n",
    "            data['peak_news_day'] = data['peak_news_count'] = data['news_per_day'] = 'N/A'\n",
    "    else:\n",
    "        data['peak_news_day'] = data['peak_news_count'] = data['news_per_day'] = 'N/A'\n",
    "\n",
    "    # ─ Social Media Note ───────────────────────────────────────────\n",
    "    social_note = safe_text(\n",
    "        driver, \"//h2[contains(text(),'Social media')]/following-sibling::p[1]\"\n",
    "    )\n",
    "    data['social_media_note'] = social_note\n",
    "\n",
    "    # ─ News Headlines ──────────────────────────────────────────────\n",
    "    headlines = []\n",
    "    for xp in [\n",
    "        \"//div[contains(@class,'media') or @id='medialist']//a[string-length(normalize-space(text()))>15]\",\n",
    "        \"//table[contains(@class,'article') or contains(@class,'news')]//a\",\n",
    "    ]:\n",
    "        items = safe_texts(driver, xp)\n",
    "        if items:\n",
    "            headlines = [h.strip() for h in items if len(h.strip()) > 15]\n",
    "            break\n",
    "\n",
    "    if not headlines:\n",
    "        soup2 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        seen  = set()\n",
    "        for a in soup2.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            text = a.get_text(' ', strip=True)\n",
    "            if (len(text) > 20 and text not in seen\n",
    "                    and (href.startswith('http') or href.startswith('/'))\n",
    "                    and not any(x in href for x in ['javascript', 'mailto', '#'])):\n",
    "                seen.add(text)\n",
    "                headlines.append(text)\n",
    "                if len(headlines) >= 25:\n",
    "                    break\n",
    "\n",
    "    data['news_headlines'] = ' | '.join(headlines[:20]) if headlines else 'N/A'\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "print('scrape_media() defined.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "## Cell 8 — Main Scraping Loop\n",
    "\n",
    "Iterates over all 4 events, scrapes Summary → Impact → Media for each, and collects results into a list of dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrape_event() defined. Ready to run.\n"
     ]
    }
   ],
   "source": [
    "def scrape_event(event: dict, headless: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Open a single GDACS event and scrape all three pages.\n",
    "\n",
    "    Each 'tab' on GDACS is actually a separate page:\n",
    "      Summary : report.aspx\n",
    "      Impact  : Earthquakes/report_shakemap.aspx\n",
    "      Media   : media.aspx\n",
    "    \"\"\"\n",
    "    urls   = build_urls(event['url'])\n",
    "    driver = create_driver(headless=headless)\n",
    "    result = {\n",
    "        'label':      event['label'],\n",
    "        'country':    event['country'],\n",
    "        'period':     event['period'],\n",
    "        'url':        event['url'],\n",
    "        'scraped_at': datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC'),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Scraping: {event['label']}\")\n",
    "\n",
    "        # ── Summary page (report.aspx) ───────────────────────────────\n",
    "        print('  → Summary page ...')\n",
    "        summary_data = scrape_summary(driver, urls['summary'])\n",
    "        result.update(summary_data)\n",
    "        print(f\"     Title     : {summary_data.get('event_title')}\")\n",
    "        print(f\"     Alert     : {summary_data.get('alert_level')} (score={summary_data.get('gdacs_score')})\")\n",
    "        print(f\"     Country   : {summary_data.get('country')}\")\n",
    "\n",
    "        # ── Impact page (report_shakemap.aspx) ───────────────────────\n",
    "        print('  → Impact page ...')\n",
    "        impact_data = scrape_impact(driver, urls['impact'])\n",
    "        result.update(impact_data)\n",
    "        print(f\"     Pop (MMI)   : {impact_data.get('exposed_population_mmi')}\")\n",
    "        print(f\"     Pop (100km) : {impact_data.get('exposed_population_100km')}\")\n",
    "        print(f\"     Tsunami     : {impact_data.get('tsunami_score')}\")\n",
    "        print(f\"     INFORM      : {impact_data.get('inform_coping_capacity')}\")\n",
    "\n",
    "        # ── Media page (media.aspx) ────────────────────────────────\n",
    "        print('  → Media page  ...')\n",
    "        media_data = scrape_media(driver, urls['media'])\n",
    "        result.update(media_data)\n",
    "        print(f\"     Total articles : {media_data.get('total_articles')}\")\n",
    "        print(f\"     Casualty art.  : {media_data.get('casualty_articles')}\")\n",
    "        print(f\"     Peak news day  : {media_data.get('peak_news_day')} ({media_data.get('peak_news_count')} articles)\")\n",
    "\n",
    "        result['scrape_status'] = 'success'\n",
    "\n",
    "    except Exception as exc:\n",
    "        result['scrape_status'] = f'error: {exc}'\n",
    "        print(f'  [ERROR] {exc}')\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print('scrape_event() defined. Ready to run.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "## Cell 9 — Run the Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Scraping: Philippines – Historical\n",
      "  → Summary page ...\n",
      "     Title     : M 6.7 in Philippines on 18 Aug 2020 00:03 UTC\n",
      "     Alert     : Orange (score=1.4)\n",
      "     Country   : Philippines\n",
      "  → Impact page ...\n",
      "     Pop (MMI)   : 11000\n",
      "     Pop (100km) : 2570000\n",
      "     Tsunami     : 0.4\n",
      "     INFORM      : 4.2 (Philippines)\n",
      "  → Media page  ...\n",
      "     Total articles : 5\n",
      "     Casualty art.  : 1 (20%)\n",
      "     Peak news day  : 21/08 (3 articles)\n",
      "\n",
      "============================================================\n",
      "Scraping: Philippines – Recent\n",
      "  → Summary page ...\n",
      "     Title     : M 6.9 in Philippines on 30 Sep 2025 13:59 UTC\n",
      "     Alert     : Orange (score=1.7)\n",
      "     Country   : Philippines\n",
      "  → Impact page ...\n",
      "     Pop (MMI)   : 860 thousand\n",
      "     Pop (100km) : N/A\n",
      "     Tsunami     : 0.6\n",
      "     INFORM      : 4.2 (Philippines)\n",
      "  → Media page  ...\n",
      "     Total articles : 1684\n",
      "     Casualty art.  : 571 (33.9%)\n",
      "     Peak news day  : 01/10 (794 articles)\n",
      "\n",
      "============================================================\n",
      "Scraping: Afghanistan – Historical\n",
      "  → Summary page ...\n",
      "     Title     : M 5.9 in Afghanistan on 21 Jun 2022 20:54 UTC\n",
      "     Alert     : Red (score=4.1)\n",
      "     Country   : Afghanistan\n",
      "  → Impact page ...\n",
      "     Pop (MMI)   : 17000\n",
      "     Pop (100km) : N/A\n",
      "     Tsunami     : N/A\n",
      "     INFORM      : 7.5 (Afghanistan)\n",
      "  → Media page  ...\n",
      "     Total articles : 1897\n",
      "     Casualty art.  : 1090 (57.5%)\n",
      "     Peak news day  : 22/06 (752 articles)\n",
      "\n",
      "============================================================\n",
      "Scraping: Afghanistan – Recent\n",
      "  → Summary page ...\n",
      "     Title     : M 6.3 in Afghanistan on 02 Nov 2025 20:29 UTC\n",
      "     Alert     : Red (score=3)\n",
      "     Country   : Afghanistan\n",
      "  → Impact page ...\n",
      "     Pop (MMI)   : 70 thousand\n",
      "     Pop (100km) : N/A\n",
      "     Tsunami     : N/A\n",
      "     INFORM      : 7.5 (Afghanistan)\n",
      "  → Media page  ...\n",
      "     Total articles : 779\n",
      "     Casualty art.  : 438 (56.2%)\n",
      "     Peak news day  : 03/11 (533 articles)\n",
      "\n",
      "============================================================\n",
      "✓ Scraping complete. 4 events processed.\n"
     ]
    }
   ],
   "source": [
    "# Set headless=False if you want to watch the browser in action\n",
    "HEADLESS = True\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for event in EVENTS:\n",
    "    record = scrape_event(event, headless=HEADLESS)\n",
    "    all_results.append(record)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Scraping complete. {len(all_results)} events processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3",
   "metadata": {},
   "source": [
    "## Cell 10 — Build & Display the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a consistent column order for the final dataset\n",
    "COLUMNS = [\n",
    "    # Metadata\n",
    "    \"label\", \"country\", \"period\", \"url\", \"scraped_at\", \"scrape_status\",\n",
    "    # Summary Tab\n",
    "    \"event_title\", \"gdacs_id\", \"magnitude\", \"depth_km\", \"lat_lon\",\n",
    "    \"event_date_utc\", \"gdacs_score\", \"alert_level\",\n",
    "    \"exposed_population_summary\",\n",
    "    # Impact Tab\n",
    "    \"impact_magnitude\", \"impact_depth_km\", \"impact_event_date_utc\",\n",
    "    \"exposed_population_mmi\", \"exposed_population_100km\",\n",
    "    \"inform_coping_capacity\", \"vulnerability_score\",\n",
    "    \"secondary_risks\", \"tsunami_score\",\n",
    "    # Media Tab\n",
    "    \"total_articles\", \"casualty_articles\", \"articles_last_hour\",\n",
    "    \"peak_news_day\", \"peak_news_count\", \"news_per_day\",\n",
    "    \"social_media_note\", \"news_headlines\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Add any missing columns as N/A\n",
    "for col in COLUMNS:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"N/A\"\n",
    "\n",
    "# Re-order to defined column order (keep any extra cols at the end)\n",
    "extra_cols = [c for c in df.columns if c not in COLUMNS]\n",
    "df = df[COLUMNS + extra_cols]\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"\\nColumn list:\")\n",
    "for c in df.columns:\n",
    "    print(f\"  {c}\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3f4a5",
   "metadata": {},
   "source": [
    "## Cell 11 — Preview Key Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3f4a5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>Philippines – Historical</td>\n",
       "      <td>Philippines – Recent</td>\n",
       "      <td>Afghanistan – Historical</td>\n",
       "      <td>Afghanistan – Recent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magnitude</th>\n",
       "      <td>6.7M</td>\n",
       "      <td>6.9M</td>\n",
       "      <td>5.9M</td>\n",
       "      <td>6.3M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alert_level</th>\n",
       "      <td>Orange</td>\n",
       "      <td>Orange</td>\n",
       "      <td>Red</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gdacs_score</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_date_utc</th>\n",
       "      <td>18 Aug 2020 00:03 UTC</td>\n",
       "      <td>30 Sep 2025 13:59 UTC</td>\n",
       "      <td>21 Jun 2022 20:54 UTC</td>\n",
       "      <td>02 Nov 2025 20:29 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exposed_population_100km</th>\n",
       "      <td>2570000</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inform_coping_capacity</th>\n",
       "      <td>4.2 (Philippines)</td>\n",
       "      <td>4.2 (Philippines)</td>\n",
       "      <td>7.5 (Afghanistan)</td>\n",
       "      <td>7.5 (Afghanistan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_articles</th>\n",
       "      <td>5</td>\n",
       "      <td>1684</td>\n",
       "      <td>1897</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>casualty_articles</th>\n",
       "      <td>1 (20%)</td>\n",
       "      <td>571 (33.9%)</td>\n",
       "      <td>1090 (57.5%)</td>\n",
       "      <td>438 (56.2%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak_news_day</th>\n",
       "      <td>21/08</td>\n",
       "      <td>01/10</td>\n",
       "      <td>22/06</td>\n",
       "      <td>03/11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak_news_count</th>\n",
       "      <td>3</td>\n",
       "      <td>794</td>\n",
       "      <td>752</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0                      1  \\\n",
       "label                     Philippines – Historical   Philippines – Recent   \n",
       "magnitude                                     6.7M                   6.9M   \n",
       "alert_level                                 Orange                 Orange   \n",
       "gdacs_score                                    1.4                    1.7   \n",
       "country                                Philippines            Philippines   \n",
       "event_date_utc               18 Aug 2020 00:03 UTC  30 Sep 2025 13:59 UTC   \n",
       "exposed_population_100km                   2570000                    N/A   \n",
       "inform_coping_capacity           4.2 (Philippines)      4.2 (Philippines)   \n",
       "total_articles                                   5                   1684   \n",
       "casualty_articles                          1 (20%)            571 (33.9%)   \n",
       "peak_news_day                                21/08                  01/10   \n",
       "peak_news_count                                  3                    794   \n",
       "\n",
       "                                                 2                      3  \n",
       "label                     Afghanistan – Historical   Afghanistan – Recent  \n",
       "magnitude                                     5.9M                   6.3M  \n",
       "alert_level                                    Red                    Red  \n",
       "gdacs_score                                    4.1                      3  \n",
       "country                                Afghanistan            Afghanistan  \n",
       "event_date_utc               21 Jun 2022 20:54 UTC  02 Nov 2025 20:29 UTC  \n",
       "exposed_population_100km                       N/A                    N/A  \n",
       "inform_coping_capacity           7.5 (Afghanistan)      7.5 (Afghanistan)  \n",
       "total_articles                                1897                    779  \n",
       "casualty_articles                     1090 (57.5%)            438 (56.2%)  \n",
       "peak_news_day                                22/06                  03/11  \n",
       "peak_news_count                                752                    533  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "KEY_COLS = [\n",
    "    \"label\", \"magnitude\", \"alert_level\", \"gdacs_score\", \"country\",\n",
    "    \"event_date_utc\", \"exposed_population_100km\",\n",
    "    \"inform_coping_capacity\", \"total_articles\", \"casualty_articles\",\n",
    "    \"peak_news_day\", \"peak_news_count\",\n",
    "]\n",
    "\n",
    "display_cols = [c for c in KEY_COLS if c in df.columns]\n",
    "display(df[display_cols].T)  # Transpose for easy reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c7",
   "metadata": {},
   "source": [
    "## Cell 12 — Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV saved to: /Users/krishhiv/Desktop/Spring 2026/DSM/Assignment 1/scraping/gdacs_earthquake_data.csv\n",
      "  Rows: 4,  Columns: 32\n",
      "  Validation read-back OK — 4 rows, 32 cols\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                   label magnitude alert_level  total_articles\\nPhilippines – Historical      6.7M      Orange               5\\n    Philippines – Recent      6.9M      Orange            1684\\nAfghanistan – Historical      5.9M         Red            1897\\n    Afghanistan – Recent      6.3M         Red             779'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✓ CSV saved to: {OUTPUT_CSV}\")\n",
    "print(f\"  Rows: {len(df)},  Columns: {len(df.columns)}\")\n",
    "\n",
    "# Quick validation\n",
    "df_check = pd.read_csv(OUTPUT_CSV)\n",
    "print(f\"  Validation read-back OK — {df_check.shape[0]} rows, {df_check.shape[1]} cols\")\n",
    "df_check[[\"label\", \"magnitude\", \"alert_level\", \"total_articles\"]].to_string(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9",
   "metadata": {},
   "source": [
    "## Cell 13 — Quick Summary Print\n",
    "\n",
    "Human-readable per-event summary for a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d8e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "───────────────────────────────────────────────────────\n",
      "Philippines – Historical\n",
      "───────────────────────────────────────────────────────\n",
      "  Event Title   : M 6.7 in Philippines on 18 Aug 2020 00:03 UTC\n",
      "  GDACS ID      : EQ 1230629\n",
      "  Magnitude     : 6.7M\n",
      "  Depth         : 10 Km\n",
      "  Event Date    : 18 Aug 2020 00:03 UTC\n",
      "  Alert Level   : Orange (score = 1.4)\n",
      "  Country       : Philippines\n",
      "\n",
      "  --- Impact ---\n",
      "  Pop in MMI    : 11000\n",
      "  Pop w/in 100km: 2570000\n",
      "  INFORM Coping : 4.2 (Philippines)\n",
      "  Vulnerability : N/A\n",
      "  Tsunami Score : 0.4\n",
      "  Secondary Risk: N/A\n",
      "\n",
      "  --- Media ---\n",
      "  Total Articles: 5\n",
      "  Casualty Art. : 1 (20%)\n",
      "  Art. Last Hour: 0\n",
      "  Peak News Day : 21/08 (3 articles)\n",
      "\n",
      "  Scrape Status : success\n",
      "\n",
      "\n",
      "───────────────────────────────────────────────────────\n",
      "Philippines – Recent\n",
      "───────────────────────────────────────────────────────\n",
      "  Event Title   : M 6.9 in Philippines on 30 Sep 2025 13:59 UTC\n",
      "  GDACS ID      : EQ 1502713\n",
      "  Magnitude     : 6.9M\n",
      "  Depth         : 10 Km\n",
      "  Event Date    : 30 Sep 2025 13:59 UTC\n",
      "  Alert Level   : Orange (score = 1.7)\n",
      "  Country       : Philippines\n",
      "\n",
      "  --- Impact ---\n",
      "  Pop in MMI    : 860 thousand\n",
      "  Pop w/in 100km: N/A\n",
      "  INFORM Coping : 4.2 (Philippines)\n",
      "  Vulnerability : N/A\n",
      "  Tsunami Score : 0.6\n",
      "  Secondary Risk: N/A\n",
      "\n",
      "  --- Media ---\n",
      "  Total Articles: 1684\n",
      "  Casualty Art. : 571 (33.9%)\n",
      "  Art. Last Hour: 0\n",
      "  Peak News Day : 01/10 (794 articles)\n",
      "\n",
      "  Scrape Status : success\n",
      "\n",
      "\n",
      "───────────────────────────────────────────────────────\n",
      "Afghanistan – Historical\n",
      "───────────────────────────────────────────────────────\n",
      "  Event Title   : M 5.9 in Afghanistan on 21 Jun 2022 20:54 UTC\n",
      "  GDACS ID      : EQ 1327560\n",
      "  Magnitude     : 5.9M\n",
      "  Depth         : 10 Km\n",
      "  Event Date    : 21 Jun 2022 20:54 UTC\n",
      "  Alert Level   : Red (score = 4.1)\n",
      "  Country       : Afghanistan\n",
      "\n",
      "  --- Impact ---\n",
      "  Pop in MMI    : 17000\n",
      "  Pop w/in 100km: N/A\n",
      "  INFORM Coping : 7.5 (Afghanistan)\n",
      "  Vulnerability : N/A\n",
      "  Tsunami Score : N/A\n",
      "  Secondary Risk: N/A\n",
      "\n",
      "  --- Media ---\n",
      "  Total Articles: 1897\n",
      "  Casualty Art. : 1090 (57.5%)\n",
      "  Art. Last Hour: 0\n",
      "  Peak News Day : 22/06 (752 articles)\n",
      "\n",
      "  Scrape Status : success\n",
      "\n",
      "\n",
      "───────────────────────────────────────────────────────\n",
      "Afghanistan – Recent\n",
      "───────────────────────────────────────────────────────\n",
      "  Event Title   : M 6.3 in Afghanistan on 02 Nov 2025 20:29 UTC\n",
      "  GDACS ID      : EQ 1508467\n",
      "  Magnitude     : 6.3M\n",
      "  Depth         : 28 Km\n",
      "  Event Date    : 02 Nov 2025 20:29 UTC\n",
      "  Alert Level   : Red (score = 3)\n",
      "  Country       : Afghanistan\n",
      "\n",
      "  --- Impact ---\n",
      "  Pop in MMI    : 70 thousand\n",
      "  Pop w/in 100km: N/A\n",
      "  INFORM Coping : 7.5 (Afghanistan)\n",
      "  Vulnerability : N/A\n",
      "  Tsunami Score : N/A\n",
      "  Secondary Risk: N/A\n",
      "\n",
      "  --- Media ---\n",
      "  Total Articles: 779\n",
      "  Casualty Art. : 438 (56.2%)\n",
      "  Art. Last Hour: 0\n",
      "  Peak News Day : 03/11 (533 articles)\n",
      "\n",
      "  Scrape Status : success\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.iterrows():\n",
    "    print(f\"\"\"\n",
    "{'─'*55}\n",
    "{row['label']}\n",
    "{'─'*55}\n",
    "  Event Title   : {row.get('event_title', 'N/A')}\n",
    "  GDACS ID      : {row.get('gdacs_id', 'N/A')}\n",
    "  Magnitude     : {row.get('magnitude', 'N/A')}\n",
    "  Depth         : {row.get('depth_km', 'N/A')}\n",
    "  Event Date    : {row.get('event_date_utc', 'N/A')}\n",
    "  Alert Level   : {row.get('alert_level', 'N/A')} (score = {row.get('gdacs_score', 'N/A')})\n",
    "  Country       : {row.get('country', 'N/A')}\n",
    "\n",
    "  --- Impact ---\n",
    "  Pop in MMI    : {row.get('exposed_population_mmi', 'N/A')}\n",
    "  Pop w/in 100km: {row.get('exposed_population_100km', 'N/A')}\n",
    "  INFORM Coping : {row.get('inform_coping_capacity', 'N/A')}\n",
    "  Vulnerability : {row.get('vulnerability_score', 'N/A')}\n",
    "  Tsunami Score : {row.get('tsunami_score', 'N/A')}\n",
    "  Secondary Risk: {row.get('secondary_risks', 'N/A')}\n",
    "\n",
    "  --- Media ---\n",
    "  Total Articles: {row.get('total_articles', 'N/A')}\n",
    "  Casualty Art. : {row.get('casualty_articles', 'N/A')}\n",
    "  Art. Last Hour: {row.get('articles_last_hour', 'N/A')}\n",
    "  Peak News Day : {row.get('peak_news_day', 'N/A')} ({row.get('peak_news_count', 'N/A')} articles)\n",
    "\n",
    "  Scrape Status : {row.get('scrape_status', 'N/A')}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}